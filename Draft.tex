\documentclass[letterpaper,11pt]{article}
%--------------Document configuration begins--------------
\usepackage{fullpage}
\usepackage[numbers]{natbib} 	%\citet{jon90} --> Jones et al. [21], \citep{jon90} --> [21]
\usepackage[colorlinks]{hyperref}
%--------------Document configuration ends--------------

\usepackage{microtype}

\usepackage{commandsrm}
\usepackage{commands_theo} 			%Theorems, lemmas, proofs, etc
%Load cleveref last
\usepackage[capitalize]{cleveref}


\bibliographystyle{plain}

\title{\vspace{-1cm} \bf Approximate DP and Regret Analysis   \vspace{-1.3cm}}
\author{}
\begin{document}
\maketitle

\section{Probing}

\paragraph{Problem Statement}
At each time $t$, an arrival is drawn from a set of $n$ types, such that $j\in[n]$ arrives with probability $p_j$.
An arrival can be \emph{active or inactive} with a known probability; inactive arrivals generate no reward, but do consume resources.
A type $j$ has an associated reward $r_j\geq 0$ and a probability $q_j\in [0,1]$: Conditioned on being of type $j$, the reward of an arrival is $r_j\cdot \Ber(q_j)$.
The decision-maker is given hiring and probing budgets $B_h,B_p\in\N$.
If he chooses to probe an arrival, then he can see the realization of the Bernoulli before accepting, thus protecting him against inactive arrivals.
He can also choose to accept the arrival without probing, in such case the reward is random, but it does not consume probing budget.

Even with full knowledge of the arrivals $Z_j$, the problem of adaptively choosing which arrivals to probe or accept is difficult.
Indeed, the state space has size $\prod_jZ_j = \Omega(\T^n)$.
This is the reason why the prevalent focus in recent research has been to bound the adaptivity gap first and then search for simpler policies \cite{adaptivity_gaps}.

We define \off to be the agent who knows the arrival process $Z(t)$, but does not know the result of the Bernoulli trials.
The value function $\voff(t,b_h,b_p)$ is determined by the natural Bellman Equations.
We start by providing an upper bound to the value function, defined by the following decision variables representing total numbers, not binary values: $y_j$ is the number of type $j$ we probed, $x_{ja}$ is the number of type $j$ accepted without probing and $x_{jp}$ is the number of type $j$ accepted after probing.

\begin{proposition}\label[proposition]{prop:probing_relaxation}
For any $b_h,b_p\in\N$, $t\in[\T]$ and realization $Z$ of arrivals, $\voff(t,b_h,b_p)$ is upper bounded by
\begin{equation}\label{eq:probing_relaxation}
\begin{array}{rrll}
(P^\star)\quad \max  & \sum_jr_j(x_{jp}+q_jx_{ja}) \\
\text{s.t.}& \sum_j y_j & \leq b_p \\
& \sum_j(x_{jp}+x_{ja}) &\leq b_h \\
&  x_{ja} + y_j & \leq Z_j(t)  &\forall j\in [n] \\
& x_{jp} &\leq q_jy_j & \forall j \in [n] \\
& y_j,x_{j,a},x_{j,p}&\geq 0 & \forall j\in[n].
\end{array}
\end{equation}
\end{proposition}	
\begin{proof}
Consider any policy for \off which determines when to probe, accept or reject.
The policy induces a random trajectory, which is determined by the realization of the Bernoulli variables, not the arrivals, since the later are given.
Define the following random variables, counting the number of times in which a type $j$: was probed, denoted $Y_j$; was accepted without probing, $X_{ja}$; and was accepted after probing, $X_{jp}$.
Finally, let $Y_{j}^+,X_{ja}^+$ be the number of active from among those probed or accepted, respectively.
We assume w.l.o.g.\ that, among the probed elements, \off takes only active ones.

Our aim is to argue that, replacing the aforementioned r.v.\ with their expectations, yields a feasible solution to $(P^\star)$.
It holds $\voff(t,b_h,b_p) = \E[\sum_j r_j(X_{jp}+X_{ja}^+)]$ and $\E[X_{ja}^+|X_{ja}]=q_jX_{ja}$, thus
\[
\voff(t,b_h,b_p) = \sum_jr_j(\E[X_{jp}]+q_j\E[X_{ja}]).
\] 

With the exception of the constraint $x_{jp}\leq q_jy_j$, $X_{ja},Y_j,X_{jp}$ satisfy a.s.\ all the constraints of $(P^\star)$, hence their expectations do too.
Observe that $X_{jp}\leq Y_{j}^+$ and $\E[Y_j^+]=q_j\E[Y_j]$, thus $\E[X_{jp}]\leq q_j\E[Y_j]$.
To summarize, $\voff(t,b_h,b_p)$ equals the value of the feasible solution given by the expectations, so $v(P^\star)$ can be only larger.
\end{proof}

\section{Probing with Multiple Values and Formulation as a MCDP}

\paragraph{Problem Statement}
Same as before, but now a type $j$ is associated with values $r_{j1},r_{j2}>0$ and $q_{j1},q_{j2}\in[0,1]$ such that $q_{j1}+q_{j2}\leq 1$.
The reward of a type $j$ is $r_{jk}$ with probability $q_{jk}$, $k\in\crl{1,2}$, and $0$ with probability $q_{j0}\defeq1-q_{j1}-q_{j2}$.
There is a conceptual gap from this case with three values and the binary (active/inactive), but the extension from three to multiple values is straightforward.
We denote $\bar r_j\defeq r_{j1}q_{j1}+r_{j2}q_{j2}$.

Let us define an \emph{input process} indexed by $t\in[2T]$.
At even times, the input reveals the type $j$ and at odd times it reveals the actual realization given probabilities $q_{jk}$.
Formally, let $(\xit)$ be a Markov chain in the state space $[n]\cup\crl{(j,k):j\in[n],k=0,1,2}$.
The transition probabilities are given by $P_{j,(j,k)}=q_{jk}$, $P_{(j,k),j'}=p_{j'}$.
For $\xi^{2t-1}=(j,k)$ we define $r(\xi^{2t-1})=r_{jk}$.

The state space for our problem can be described as $\S=\crl{(b_h,b_p),(b_h,b_p,\diamond):b_h,b_p\in\N,\diamond\in \crl{\texttt{a,p,r}}}$, see \cref{fig:probing_states}.
The component $\diamond$ indicates if we are in the accept, probe or reject stage.
There is never a $\diamond$ at times $2t$ and, at $2t-1$, feasible controls are different for distinct $\diamond$.

\begin{figure}
\centering
\scalebox{0.9}{%
\input{img/probing_states}
}
\caption{Transitions for the online probing problem. 
Numbers below the arrows represent the reward of a transition.
At time $2t$ the possible actions are: accept, probe and reject.
At $2t-1$, depending on $\diamond$, we can either accept or reject and $b_h$ may be discounted.
}
\label{fig:probing_states}
\end{figure}

\section{Relaxed Bellman Equations}

Consider a DP with state space $\S$ and a sequence of inputs $\xit\in\Xi$ for $t\in[\T]$, where $t$ is the \emph{time to-go}.
We assume $\xit$ evolves as a Markov chain on state space $\Xi$, with known transition matrix and initial distribution.

For every state $s\in\S$, arrival $\xi$ and control $u\in\U$, we  collect a reward $\Re(s,\xi,u)$, which could be $-\infty$, and transition to the state $\Tr(s,\xi,u)$.
Observe that the reward function is deterministic, but the input $\xi$ is random, thus making the reward itself at each time a random variable.
We stress that, at each time $t$, the decision-maker first observes $\xit$ and based on that makes a decision.
\anote{
The only difference between this formulation and MCDP is that in MCDP the reward depends on a pair of states (current and transition), but I think that would be redundant.
}
Any time dependence will be embedded in the process $\xit$, e.g., if the randomness represents unknown demand, the state of the Markov chain could be of the form $\J=\crl{(t,x):t\in[\T],x\in\Rp}$, where the first component gives us the time and the second the demand for the period.
This formulation is known as a Markov Chance-Decision Process \cite[chap 13]{online_book}.

Given this formulation, if at time $t$ the current state is $s\in\S$, we obtain the Bellman \cref{eq:bellman_mcdp}
\begin{equation}\label{eq:bellman_mcdp}
v(t,s,\xit) = \max_{u\in\U}\crl{\Re(s,\xit,u) +\E_{\xi^{t-1}}[v(t-1,\Tr(s,\xit,u),\xi^{t-1})|\xit]},
\end{equation}
with boundary condition $v(0,\cdot,\cdot)=0$ and $\xi^0$ set to an arbitrary value.
This is a well-known fact; contingent on the actions being optimal, the total to-go value can be decomposed as instant reward plus future value.
We will now state a relaxed version of \cref{eq:bellman_mcdp}, where the main idea is to replace $v$, which is difficult to compute, for an efficiently computable upper bound $\tilde v$.
If $\tilde v$ satisfies a probabilistic version of a Bellman Equation, then the process of iteratively solving the relaxed problem can be justified.

\begin{definition}[Relaxed Bellman Equations]
The function $\varphi:[T]\times\S\times\Xi\to \R$ satisfies the relaxed Bellman equations w.r.t.\ $v$ if:
\begin{enumerate}
\item Initial condition: $\E[v(\T,S^\T,\xi^\T)] \leq \E[\varphi(\T,S^\T,\xi^\T)]$
\item Probabilistic inequality: $\forall s\in\S,t\in[T]$, there is a set $\calB(t,s)\subseteq\Omega$ such that
\begin{equation}\label{eq:relaxed_bellman}
\varphi(t,s,\xit) \leq \max_{u\in\U}\crl{R(s,\xit,u)+\E_{\xi^{t-1}}[\varphi(t-1,\Tr(s,\xit,u),\xi^{t-1})|\xit]} \quad \forall \omega\in\calB(t,s).
\end{equation}
\end{enumerate}
\end{definition} 
 
\begin{example}
Consider a knapsack problem with two types of stochastic arrivals.
Type $j$ has reward $r_j$, arrives at each period with probability $p_j$ and uses space $w_j$ in the knapsack.
Assume all the arrivals are revealed.
We will show that the following satisfies the relaxed Bellman equations:
\[
\varphi(t,b,\xit) = \max\crl*{\sum_{j}x_jr_j:\sum_{j}w_jx_j\leq b, \quad 0\leq x_j \leq Z_j(t) \quad \forall j\in[2]}.
\]
The initial condition is trivial, since we are relaxing the integrality constraints.
We are left to show the probabilistic inequality.
Let $X[\omega]$ be a solution to the LP and let $i=\xit$.
For the remainder of the example, we will assume $X_i\in(0,1)$.
If $Z_i(t)\geq 2$,  then an exchange argument shows
\[
\varphi(t,b,\xit) = \max_{u=\texttt{r,a}}\crl{r(b,\xit,u)+\varphi(t-1,b\ominus u,\xi^{t-1})}
= \varphi(t-1,b,\xi^{t-1}),
\]
where $b\ominus u$ denotes the natural reduction of budget of an action.
We conclude that, if $Z_i(t)\geq 2$ and $X_i<1$, the probabilistic inequality is satisfied a.s.
We are left with the case $Z_i(t)=1$ (the case $Z_i(t)=0$ is impossible when $X_i\in (0,1)$).
In this case, 
\[
\varphi(t,b,\xit) > \max_{u=\texttt{r,a}}\crl{r(b,\xit,u)+\varphi(t-1,b\ominus u,\xi^{t-1})}.
\]
We conclude that the probabilistic inequality is satisfied in $\calB(t,b)=\crl{\omega\in\Omega:Z_i(t)\geq 2}$.
Observe that the probability of this set is exponentially close to one.
Indeed, $\Pr[\calB(t,b)]=1-tp_i(1-p_i)^{t-1}$.
\end{example}

\begin{proposition}\label[proposition]{prop:relaxed_bellman}
Assume $\varphi$ satisfies the relaxed Bellman equations w.r.t.\ $v$.
Denote $\St$ the state of the policy that takes action $\Ut=\Ut(\xit,\St)$, where $\Ut$ is a maximizer in \cref{eq:relaxed_bellman}.
Then,
\[
\E[v(\T,S^\T,\xi^\T)] \leq \E\brk*{ \sum_{t=1}^\T (\Re(\St,\xit,\Ut)+\rmax\Ins{B(t,\St)})}. 
\]
\end{proposition}
\begin{proof}
By the initial condition property, $\E[v(\T,S^\T,\xi^\T)] \leq \E[\varphi(\T,S^\T,\xi^\T)]$.
It is now enough to argue the following equation and conclude by a simple induction step.
\[
\varphi(t,\St,\xit)\leq \E_{\xi^{t-1}}[\Re(\St,\xit,\Ut)+\varphi(t-1,S^{t-1},\xi^{t-1}) +  \rmax\Ins{B(t,\St)}|\xit].
\]
By definition of $\Ut$, $\varphi(t,\St,\xit)\leq \Re(\St,\xit,\Ut)+\E_{\xi^{t-1}}[\varphi(t-1,S^{t-1},\xi^{t-1})] +  \rmax\Ins{B(t,\St)}$.
Since $\Re(\St,\xit,\Ut)$ and $\Ins{B(t,\St)}$ are measurable with respect to $\xit$, they can be inside the expectation, showing that the desired equation holds.
\end{proof}

\anote{The $\rmax$ term here corresponds to the loss in $\varphi$.
All the relaxations we use have the same $\rmax$ as we would expect.}


\section{Relaxation for Multiple Probing}

The natural relaxation is similar to \cref{eq:probing_relaxation}, but it presents a new challenge.
For binary cases, it was obvious that, if \off wants to probe an arrival, he will always accept it if it is active.
In this case it could be that \off probes $j$ in hope of observing $r_{j2}$ and will reject $r_{j1}$.
We must, therefore, reason about \off's actions at all times, instead of just at even times as before and concluding with structural properties.

Let $\calX\defeq\Rp^{4n}$ be the space for the decision variables.
In \cref{eq:probing_lp} we present a LP which will be the building block for our relaxation.
Note that, by definition, if $t$ is odd, $Z_j(t)=Z_j(t+1)-\In{\xi^{t+1}=j}$.

\begin{equation}\label{eq:probing_lp}
\begin{array}{rrll}
P[t,Z,b] \;= \max & \multicolumn{3}{l}{\sum_{j,k}r_{jk}(x_{(j,k)a}+q_{jk}x_{ja})} \\
\text{s.t.}& \sum_{j,k}x_{(j,k)a}+\sum_jx_{ja} &\leq b_h  \\
&  \sum_j x_{jp} & \leq b_p   \\
&  x_{ja} +x_{jp} & \leq Z_j(t)  & j\in [n] \\
& x_{(j,k)a} &\leq q_{jk}x_{jp} &  j \in [n],k\in [2]\\ 
& x&\in\calX.
\end{array}
\end{equation}

The LP in \cref{eq:probing_lp} can be interpreted as follows...\todonote

We are ready to parse the relaxation in \cref{eq:probing_relax}.
Recall that a state is of the form $s=(b_h,b_p,\diamond)$ with $\diamond\in \crl{\texttt{a},\texttt{p},\texttt{r},\varnothing}$, where $\diamond=\varnothing$ for even times.
\begin{equation}\label{eq:probing_relax}
\varphi(t,s,\xit) = \left\{ 
\begin{array}{ll}
v(P[t,Z,b]) & \diamond=\varnothing \\ 
v(P[t-1,Z,b]) & \diamond = \texttt{r} \\ 
r_{\xit}+v(P[t-1,Z,b]) & \diamond = \texttt{a} \\ 
\max\crl{r_{\xit}+v(P[t-1,Z,b-e_h]),v(P[t-1,Z,b])} & \diamond = \texttt{p}
\end{array} 
\right.
\end{equation}

\begin{lemma}
Let $\bar X\in\calX$ be a maximizer of $(P[t,Z,b])$ for $t$ even.
Say $\xit=j$, then, at time $t$, \off is satisfied with the following actions:
\begin{enumerate}
\item Accept if $\bar X_{ja}\geq 1$
\item Reject if $\bar X_{ja}+\bar X_{jp}\leq Z_j(t)-1$
\item Probe if $\bar X_{jp}\geq 1$ and then accept $(j,k)$ if $\bar X_{(j,k)a}\geq 1$
\end{enumerate}
\end{lemma}
\begin{proof}
We will show that $\varphi$ satisfies the relaxed Bellman equations, then conclude in virtue of \cref{prop:relaxed_bellman}.
The initial condition follows from the same argument as in \cref{prop:probing_relaxation}.
We are left to show the the probabilistic inequality
\[
\varphi(t,s,\xit) \leq \max_{u\in\U}\crl{R(s,\xit,u)+\E_{\xi^{t-1}}[\varphi(t-1,\Tr(s,\xit,u),\xi^{t-1})|\xit]} \quad \forall \omega\in\calB(t,s).
\]
The set $\calB(t,s)$ is defined as the $\omega\in\Omega$ such that at least one of the conditions (1),(2) or (3) are satisfied.
Observe that, since $t$ is even, the instant reward $R(s,\xit,u)$ is zero.
In cases (1) and (2) is easy to verify the probabilistic inequality.
For case (3) we apply Jensen's Inequality:
\begin{align*}
\varphi(t,s,\xit) &= v(P[t,Z,b]) \\
&= \max\crl{\E[r_{\xi^{t-1}}|\xit]+v(P[t-1,Z,b-e_p-e_h]),v(P[t-1,Z,b-e_p])} \\
&\leq \E_{\xi^{t-1}}[\max\crl{r_{\xit}+v(P[t-1,Z,b-e_h-e_p]),v(P[t-1,Z,b-e_p])}].
\end{align*}
The last term equals $R(s,\xit,u)+\E_{\xi^{t-1}}[\varphi(t-1,s\ominus\texttt{p},\xi^{t-1})|\xit]$ and the proof is complete.
\end{proof}


\section{General Version}

Iterating the Bellman \cref{eq:bellman_mcdp}, we arrive at the equivalent expression 
\begin{equation}
v(t,s,\xit) = \E_{\xi^{t-1},\ldots,\xi^1}\brk*{\max_{u^t,\ldots,u^1}\sum_{\tau=1}^t\Re(S^\tau,\xi^\tau,u^\tau) \text{ s.t. }  S^t=s \text{ and } S^{\tau-1}=\Tr(S^\tau,\xi^\tau,u^\tau) \forall \tau<t }.
\end{equation}

Even if we reveal the entire input sequence, the problem inside of the expectation could be NP-Hard.
This yields another motivation for online algorithms: by pretending the input arrives online, it gives a way of efficiently approximating a hard problem.

We can define \off as the agent that knows the entire sequence, but this is too pessimistic and in general does not lead to sensible algorithms.
Instead, we will define a hierarchy of such agents, each with more information.

We now start imposing assumptions to guarantee two things: (1) we can find a \emph{tractable relaxation} to the maximization problem and (2) the relaxation is useful for \onl.
The first point will be achieved by obtaining a concave maximization program with convex constraints, while the second requires \emph{revealing some, but not all, information} to \off.

\paragraph{Assumption: Finite Dimension}
We identify $\S\subseteq \R^\ds$ and $\U\subseteq\R^\du$.

\paragraph{Assumption: Linear Transitions}
There are operators $A,B$ such that, given a control $u^{t+1}$ at state $S^{t+1}$, the system transitions as follows
\[
S^{t} = AS^{t+1} + Bu^{t+1}, t\in[\T-1], \quad 
S^T \text{ given}.
\]
By a simple induction, given a sequence of controls $u^\tau$ for $\tau=\T,\ldots,t+1$, the state at time $t$ is
\[
S^t = A^{T-t}S^T + \sum_{\tau=t+1}^\T A^{\tau-1-t}Bu^\tau, \quad t\in[T].
\]

If $A$ is idempotent (a projection), then the formula simplifies greatly.
\anote{	For now we will assume $A$ is the identity, but the projection thing is important. The results shouldn't change.}
In this case we have the simpler formula
\begin{equation}\label{eq:linear_state}
S^t = S^T + \sum_{\tau=t+1}^\T Bu^\tau, \quad t\in[T].
\end{equation}

For notation simplicity, we assume $0\in \R^\du$ is always a valid action.
\anote{I'm not sure how restrictive this is.}

\paragraph{Assumption: Subadditve Concave Rewards}
The reward function $\Re(s,\xi,u)$ depends on $s,\xi$ only to specify feasible controls, i.e., combinations of arrivals and controls that give reward $-\infty$.
Formally, define $\U(s,\xi)\defeq \crl{u\in\U: \Re(s,\xi,u)>-\infty}$.
We assume that $\Re(s,\xi,u)$ is just a function of $u$ provided that $u\in\U(s,\xi)$.

Additionally, we assume that either (1) $\Re(u)$ is concave and subadditive or (2) there is a concave function $\tilde \Re:\U^\T\to\R$ such that $\sum_t\Re(u^t)\leq \tilde \Re(\sum_t u^t)$.

The operator $B$ is also subadditive.

\anote{In principle $\Re$ is defined only for discrete points, so we actually require an extension that takes real numbers.}

\paragraph{Assumption: Indexing for controls and states}
The hope is that we can aggregate arrivals in a convenient way.
For this aim, we need to reduce the information in $\Xi$.
We assume there is a mapping $J:\Xi\to[n]$.
Intuitively, if $\xi,\xi'\in\J$ are such that $J(\xi)=J(\xi')$, then they are identical in terms of reward and feasibility constraints.
As a shortcut, we use $\Jt\defeq J(\xit)$.

Similarly, there is a mapping $I:\S\to [m]$ and we denote $\It=I(S^t)$.
We assume there is a convex function $f:\R^\ds\times\R^\du\to\R^d$, called \emph{feasibility function}, and discrete sets $\U_{ij}$ such that
\[
\U(s,\xi) = \crl{u\in \U_{I(s),J(\xi)}: f(s,u)\geq 0}.
\]

We denote $d_{ij}\defeq \abs{U_{ij}}$ the number of available controls and enumerate its elements as $\U_{ij} = \crl{u_{ijk}: k\in [d_{ij}]}$. 
We define the processes $Z_j(t)\defeq\sum_{\tau\leq t}\In{J^\tau=j}=\sum_{\tau\leq t}\In{J(\xi^\tau)=j}$ and $Z_{ij}(t) \defeq \sum_{\tau\leq t}\In{J^\tau=j,I^\tau=i}$.

\anote{For now I'm thinking of a discrete set of controls (e.g. accept, reject) and the results will depend on the number of controls.
Nevertheless, I think there is an extension to, e.g., reject or sell $k$ units, with $k\in\N$.
It is important that the ``basis" has small dimension, rather than the controls themselves. 
}

\paragraph{Assumption: Monotonicity}
The state $S^t$ is monotone over time.
Additionally, the feasibility function $f(s,u)$ are also monotone in $s$.
\anote{This is not fundamental in the following sense: the state can be monotone by intervals, e.g., the inventory descreases in $T,\ldots,T/2$, but at $T/2$ we can restock, then it decreases again in $T/2-1,\ldots,1$.
}

Now we will separate the processes into \emph{revealed information} and its counterpart, the \emph{concealed information}.
\off will have access to the revealed information, but only distributional information for the rest.
We denote by $\Theta\subseteq [n]$ the set of indexes we reveal, meaning that \off knows $Z_j(\T)$ for $j\in\Theta$.
Below we present the natural fluid relaxation of the concealed information, which we call \emph{the canonical relaxation}.
The first set of constraints, those involving $Z_{ij}$, are referred to as \emph{dynamics equations}.
The constraints defined by $f$ are, naturally, called \emph{feasibility equations}.

\begin{definition}[Canonical Relaxation]
If $\Theta$ are the revealed processes, and we denote $Z_\Theta(T)$ as a vector with the components $j\in\Theta$, then the following is called the canonical relaxation given $\Theta$
\begin{equation}\label{eq:theta_relaxation}
\begin{array}{rrll}
(P_\Theta)\quad \max  & \tilde \Re\prn*{\sum_{i\in [m]}\sum_{k\in [k_i]}x_{k,i}u_{k,i}} \\
\text{s.t.}&  \sum_k x_{ijk} & \leq \E[Z_{ij}(T)| Z_\Theta(T) ]  &\forall i\in[m], j\in[n] \\
& f\prn*{S^\T+B\sum_{i,j,k}u_{ijk}x_{ijk},0}&\geq 0  \\
& x_{ijk}&\geq 0 & \forall i\in [m], j\in [n], k\in [d_{ij}] .
\end{array}
\end{equation}
\end{definition}

\begin{proposition}
For any choice of indexes $\Theta$, $v(T,S^T)\leq v(P_\Theta)$	
\end{proposition}

\begin{definition}[Valid Relaxation]
	TODO
\end{definition}


\bibliography{biblio}

\end{document}
