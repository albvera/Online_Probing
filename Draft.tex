\documentclass[letterpaper,11pt]{article}
%--------------Document configuration begins--------------
\usepackage{fullpage}
\usepackage[numbers]{natbib} 	%\citet{jon90} --> Jones et al. [21], \citep{jon90} --> [21]
\usepackage[colorlinks]{hyperref}
%--------------Document configuration ends--------------

\usepackage{microtype}

\usepackage{commandsrm}
\usepackage{commands_theo} 			%Theorems, lemmas, proofs, etc
%Load cleveref last
\usepackage[capitalize]{cleveref}


\bibliographystyle{plain}

\title{\vspace{-1cm} \bf Approximate DP and Regret Analysis   \vspace{-1.3cm}}
\author{}
\begin{document}
\maketitle

\section{Probing}

\paragraph{Problem Statement}
At each time $t$, an arrival is drawn from a set of $n$ types, such that $j\in[n]$ arrives with probability $p_j$.
An arrival can be \emph{active or inactive} with a known probability; inactive arrivals generate no reward, but do consume resources.
A type $j$ has an associated reward $r_j\geq 0$ and a probability $q_j\in [0,1]$: Conditioned on being of type $j$, the reward of an arrival is $r_j\cdot \Ber(q_j)$.
The decision-maker is given hiring and probing budgets $B_h,B_p\in\N$.
If he chooses to probe an arrival, then he can see the realization of the Bernoulli before accepting, thus protecting him against inactive arrivals.
He can also choose to accept the arrival without probing, in such case the reward is random, but it does not consume probing budget.

Even with full knowledge of the arrivals $Z_j$, the problem of adaptively choosing which arrivals to probe or accept is difficult.
Indeed, the state space has size $\prod_jZ_j = \Omega(\T^n)$.
This is the reason why the prevalent focus in recent research has been to bound the adaptivity gap first and then search for simpler policies \cite{adaptivity_gaps}.

We define \off to be the agent who knows the arrival process $Z(t)$, but does not know the result of the Bernoulli trials.
The value function $\voff(t,b_h,b_p)$ is determined by the natural Bellman Equations.
We start by providing an upper bound to the value function, defined by the following decision variables representing total numbers, not binary values: $y_j$ is the number of type $j$ we probed, $x_{ja}$ is the number of type $j$ accepted without probing and $x_{jp}$ is the number of type $j$ accepted after probing.









\section{Stochastic Knapsack With Bayesian Learning}

\paragraph{Problem Statement}
Items with known reward $r$ and random weight $W$ can be placed in a knapsack of capacity $B$.
At every time $t$, an arrival $j\in[n]$ is drawn with some known distribution, which generates a known reward of $r_j$.
The weight $W_j$ is a random variable, which is revealed only after the decision of accept/reject has been made.
An arrival can be accepted only if the weight is a.s.\ smaller than the remaining capacity.
Furthermore, the distribution of $W_j$ depends on an unknown parameter $\theta_j$ in the parametric form $W_k|\theta_j\sim F_{\theta_j}$.
Before the process starts, we are given a prior distribution $\Theta_j$ for the parameter of each type.

We assume that, at the end of each period, we observe the realization of both accepted and rejected items.
Let $\calF_t$ be the $\sigma$-algebra generated by the process, which includes the observations of weights up to, and including, time $t$.
We define $\calF_{T+1}\defeq\varnothing$.
We assume the updated average weight $\hat W_{jt} \defeq  \E[W_j|\calF_{t+1}]$ can be computed efficiently.

The intuition behind our relaxation is as follows.
At any time $t$, we give more power to the decision maker by (1) revealing the future arrivals $Z(t)$ and (2) replacing the items by deterministic copies having weight equal to $\hat W_{tj}$, instead of random.
The relaxation is presented in \cref{eq:knapsack_lp}.
Observe that the dependence in the arrival $\xit$ is ``absorbed'' in $Z(t)$.

\begin{equation}\label{eq:knapsack_lp}
\begin{array}{rrll}
\varphi(t,b,\xit) \; = \; \max & \multicolumn{3}{l}{\sum_{j}r_{j}x_j} \\
\text{s.t.}& \sum_{j}\hat W_{jt}x_j  &\leq b  \\
&  x_j & \leq Z_j(t)  & j\in [n] \\
& x&\geq 0.
\end{array}
\end{equation}




\section{General Version}

Iterating the Bellman \cref{eq:bellman_mcdp}, we arrive at the equivalent expression 
\begin{equation}
v(t,s,\xit) = \E_{\xi^{t-1},\ldots,\xi^1}\brk*{\max_{u^t,\ldots,u^1}\sum_{\tau=1}^t\Re(S^\tau,\xi^\tau,u^\tau) \text{ s.t. }  S^t=s \text{ and } S^{\tau-1}=\Tr(S^\tau,\xi^\tau,u^\tau) \forall \tau<t }.
\end{equation}

Even if we reveal the entire input sequence, the problem inside of the expectation could be NP-Hard.
This yields another motivation for online algorithms: by pretending the input arrives online, it gives a way of efficiently approximating a hard problem.

We can define \off as the agent that knows the entire sequence, but this is too pessimistic and in general does not lead to sensible algorithms.
Instead, we will define a hierarchy of such agents, each with more information.

We now start imposing assumptions to guarantee two things: (1) we can find a \emph{tractable relaxation} to the maximization problem and (2) the relaxation is useful for \onl.
The first point will be achieved by obtaining a concave maximization program with convex constraints, while the second requires \emph{revealing some, but not all, information} to \off.

\paragraph{Assumption: Finite Dimension}
We identify $\S\subseteq \R^\ds$ and $\U\subseteq\R^\du$.

\paragraph{Assumption: Linear Transitions}
There are operators $A,B$ such that, given a control $u^{t+1}$ at state $S^{t+1}$, the system transitions as follows
\[
S^{t} = AS^{t+1} + Bu^{t+1}, t\in[\T-1], \quad 
S^T \text{ given}.
\]
By a simple induction, given a sequence of controls $u^\tau$ for $\tau=\T,\ldots,t+1$, the state at time $t$ is
\[
S^t = A^{T-t}S^T + \sum_{\tau=t+1}^\T A^{\tau-1-t}Bu^\tau, \quad t\in[T].
\]

If $A$ is idempotent (a projection), then the formula simplifies greatly.
\anote{	For now we will assume $A$ is the identity, but the projection thing is important. The results shouldn't change.}
In this case we have the simpler formula
\begin{equation}\label{eq:linear_state}
S^t = S^T + \sum_{\tau=t+1}^\T Bu^\tau, \quad t\in[T].
\end{equation}

For notation simplicity, we assume $0\in \R^\du$ is always a valid action.
\anote{I'm not sure how restrictive this is.}

\paragraph{Assumption: Subadditve Concave Rewards}
The reward function $\Re(s,\xi,u)$ depends on $s,\xi$ only to specify feasible controls, i.e., combinations of arrivals and controls that give reward $-\infty$.
Formally, define $\U(s,\xi)\defeq \crl{u\in\U: \Re(s,\xi,u)>-\infty}$.
We assume that $\Re(s,\xi,u)$ is just a function of $u$ provided that $u\in\U(s,\xi)$.

Additionally, we assume that either (1) $\Re(u)$ is concave and subadditive or (2) there is a concave function $\tilde \Re:\U^\T\to\R$ such that $\sum_t\Re(u^t)\leq \tilde \Re(\sum_t u^t)$.

The operator $B$ is also subadditive.

\anote{In principle $\Re$ is defined only for discrete points, so we actually require an extension that takes real numbers.}

\paragraph{Assumption: Indexing for controls and states}
The hope is that we can aggregate arrivals in a convenient way.
For this aim, we need to reduce the information in $\Xi$.
We assume there is a mapping $J:\Xi\to[n]$.
Intuitively, if $\xi,\xi'\in\J$ are such that $J(\xi)=J(\xi')$, then they are identical in terms of reward and feasibility constraints.
As a shortcut, we use $\Jt\defeq J(\xit)$.

Similarly, there is a mapping $I:\S\to [m]$ and we denote $\It=I(S^t)$.
We assume there is a convex function $f:\R^\ds\times\R^\du\to\R^d$, called \emph{feasibility function}, and discrete sets $\U_{ij}$ such that
\[
\U(s,\xi) = \crl{u\in \U_{I(s),J(\xi)}: f(s,u)\geq 0}.
\]

We denote $d_{ij}\defeq \abs{U_{ij}}$ the number of available controls and enumerate its elements as $\U_{ij} = \crl{u_{ijk}: k\in [d_{ij}]}$. 
We define the processes $Z_j(t)\defeq\sum_{\tau\leq t}\In{J^\tau=j}=\sum_{\tau\leq t}\In{J(\xi^\tau)=j}$ and $Z_{ij}(t) \defeq \sum_{\tau\leq t}\In{J^\tau=j,I^\tau=i}$.

\anote{For now I'm thinking of a discrete set of controls (e.g. accept, reject) and the results will depend on the number of controls.
Nevertheless, I think there is an extension to, e.g., reject or sell $k$ units, with $k\in\N$.
It is important that the ``basis" has small dimension, rather than the controls themselves. 
}

\paragraph{Assumption: Monotonicity}
The state $S^t$ is monotone over time.
Additionally, the feasibility function $f(s,u)$ are also monotone in $s$.
\anote{This is not fundamental in the following sense: the state can be monotone by intervals, e.g., the inventory descreases in $T,\ldots,T/2$, but at $T/2$ we can restock, then it decreases again in $T/2-1,\ldots,1$.
}

Now we will separate the processes into \emph{revealed information} and its counterpart, the \emph{concealed information}.
\off will have access to the revealed information, but only distributional information for the rest.
We denote by $\Theta\subseteq [n]$ the set of indexes we reveal, meaning that \off knows $Z_j(\T)$ for $j\in\Theta$.
Below we present the natural fluid relaxation of the concealed information, which we call \emph{the canonical relaxation}.
The first set of constraints, those involving $Z_{ij}$, are referred to as \emph{dynamics equations}.
The constraints defined by $f$ are, naturally, called \emph{feasibility equations}.

\begin{definition}[Canonical Relaxation]
If $\Theta$ are the revealed processes, and we denote $Z_\Theta(T)$ as a vector with the components $j\in\Theta$, then the following is called the canonical relaxation given $\Theta$
\begin{equation}\label{eq:theta_relaxation}
\begin{array}{rrll}
(P_\Theta)\quad \max  & \tilde \Re\prn*{\sum_{i\in [m]}\sum_{k\in [k_i]}x_{k,i}u_{k,i}} \\
\text{s.t.}&  \sum_k x_{ijk} & \leq \E[Z_{ij}(T)| Z_\Theta(T) ]  &\forall i\in[m], j\in[n] \\
& f\prn*{S^\T+B\sum_{i,j,k}u_{ijk}x_{ijk},0}&\geq 0  \\
& x_{ijk}&\geq 0 & \forall i\in [m], j\in [n], k\in [d_{ij}] .
\end{array}
\end{equation}
\end{definition}

\begin{proposition}
For any choice of indexes $\Theta$, $v(T,S^T)\leq v(P_\Theta)$	
\end{proposition}

\begin{definition}[Valid Relaxation]
	TODO
\end{definition}


\bibliography{biblio}

\end{document}
